import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import time
import io
import datetime

# è¨­å®šç¶²é è³‡è¨Š
st.set_page_config(page_title="æ¾³é–€æ—¥å ±ä¸‹è¼‰å™¨", page_icon="ğŸ‡²ğŸ‡´")

def start_full_crawler(target_url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0"
    }

    try:
        res = requests.get(target_url, headers=headers, timeout=15)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # æå–æ–‡ç« é€£çµ
        links = []
        for a in soup.find_all('a', href=True):
            if 'content_' in a['href']:
                links.append(urljoin(target_url, a['href']))
        
        article_links = list(dict.fromkeys(links))
        total = len(article_links)
        
        if total == 0:
            st.error("âŒ æ‰¾ä¸åˆ°æ–‡ç« é€£çµï¼Œè«‹æª¢æŸ¥ç¶²å€ã€‚")
            return None

        # æº–å‚™é€²åº¦æ¢
        progress_bar = st.progress(0)
        status_text = st.empty()

        # æª”åè™•ç†
        date_match = re.search(r'(\d{4}-\d{2}/\d{2})', target_url)
        date_id = date_match.group(1).replace('-', '').replace('/', '') if date_match else "Archive"

        # HTML æ¨¡æ¿
        html_start = f"""<!DOCTYPE html><html lang="zh-Hant"><head><meta charset="UTF-8">
        <style>
            body {{ font-family: sans-serif; line-height: 1.8; max-width: 800px; margin: 0 auto; padding: 20px; background: #f4f4f4; }}
            .article-card {{ background: white; padding: 30px; margin-bottom: 30px; border-radius: 8px; shadow: 0 2px 5px rgba(0,0,0,0.1); }}
            .news-title {{ color: #aa0000; font-size: 1.8em; font-weight: bold; }}
            .news-image {{ max-width: 100%; display: block; margin: 20px auto; border-radius: 4px; }}
            #toc {{ background: white; padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid #ddd; }}
        </style></head><body><h1>æ¾³é–€æ—¥å ±åˆè¼¯ ({date_id})</h1><div id="toc"><h3>ğŸ“‹ ç›®éŒ„</h3>"""

        toc_html = ""
        articles_body = ""

        for i, link in enumerate(article_links):
            try:
                status_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {link.split('/')[-1]}")
                r = requests.get(link, headers=headers, timeout=10)
                r.encoding = 'utf-8'
                raw_html = r.text

                # æ–¹æ­£æ¨™ç±¤æå–æ¨™é¡Œ
                title_match = re.search(r'<founder-title>(.*?)</founder-title>', raw_html, re.DOTALL)
                final_title = title_match.group(1).strip() if title_match else "ç„¡æ¨™é¡Œ"
                final_title = final_title.replace('<![CDATA[', '').replace(']]>', '')

                a_soup = BeautifulSoup(raw_html, 'html.parser')
                
                imgs_html = ""
                for img in a_soup.find_all('img'):
                    src = img.get('src')
                    if src and '/res/' in src:
                        full_img_url = urljoin(link, src)
                        imgs_html += f'<img src="{full_img_url}" class="news-image">'

                content_div = a_soup.find(id="ozoom")
                content_html = str(content_div) if content_div else "<p>ï¼ˆå…§æ–‡æ“·å–å¤±æ•—ï¼‰</p>"

                anchor_id = f"news_{i}"
                toc_html += f'<a href="#{anchor_id}" style="display:block;margin:5px 0;text-decoration:none;color:#0056b3;">{i+1}. {final_title}</a>'
                articles_body += f'<div class="article-card" id="{anchor_id}"><div class="news-title">{final_title}</div><hr>{imgs_html}<div>{content_html}</div></div>'
                
                progress_bar.progress((i + 1) / total)
                time.sleep(0.1)
            except:
                continue

        status_text.text("âœ¨ è™•ç†å®Œæˆï¼è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ä¸‹è¼‰ã€‚")
        return html_start + toc_html + "</div>" + articles_body + "</body></html>"

    except Exception as e:
        st.error(f"å´©æ½°: {e}")
        return None


# --- UI ä»‹é¢ ---
st.title("ğŸ‡²ğŸ‡´ æ¾³é–€æ—¥å ±å…¨ç‰ˆé¢ä¸‹è¼‰å™¨ v.0.2")
st.info("æ‚¨å¯ä»¥æ‰‹å‹•è¼¸å…¥ç¶²å€ï¼Œæˆ–é»æ“Šä¸‹æ–¹æŒ‰éˆ•ç›´æ¥æŠ“å–ä»Šå¤©çš„å ±ç´™ã€‚")

# 1. å»ºç«‹å…©æ¬„ä½ˆå±€ï¼Œè®“æŒ‰éˆ•çœ‹èµ·ä¾†æ›´æ•´é½Š
col1, col2 = st.columns([1, 1])

with col1:
    # ç²å–ä»Šå¤©æ—¥æœŸçš„é‚è¼¯
    today = datetime.date.today()
    # æ ¼å¼åŒ–ç‚ºç¶²å€è¦æ±‚çš„æ¨£å¼ï¼šYYYY-MM/DD
    formatted_date = today.strftime("%Y-%m/%d")
    today_url = f"https://www.macaodaily.com/html/{formatted_date}/node_1.htm"
    
    if st.button("ğŸ“… ä¸‹è¼‰ç•¶å¤©æ–°è", use_container_width=True):
        url_input = today_url # é‡å¯« url_input
        st.session_state['run_url'] = today_url # å­˜å…¥ session è§¸ç™¼åŸ·è¡Œ

with col2:
    if st.button("ğŸ§¹ æ¸…é™¤è¼¸å…¥", use_container_width=True):
        st.session_state.pop('run_url', None)

# 2. æ‰‹å‹•è¼¸å…¥æ¡†ï¼ˆçµ¦äºˆé è¨­å€¼æˆ–é¡¯ç¤ºè‡ªå‹•ç”Ÿæˆçš„ç¶²å€ï¼‰
default_val = st.session_state.get('run_url', today_url)
url_to_process = st.text_input("ç‰ˆé¢ç¶²å€:", value=default_val)

# 3. åŸ·è¡Œé‚è¼¯
# å¦‚æœé»æ“Šäº†ã€Œä¸‹è¼‰ç•¶å¤©æ–°èã€æˆ–è€…æ‰‹å‹•é»æ“Šã€Œé–‹å§‹åˆ†æã€
if st.button("ğŸš€ é–‹å§‹åˆ†æä¸¦ç”Ÿæˆåˆè¼¯", type="primary"):
    if url_to_process:
        with st.spinner(f'æ­£åœ¨æ¬é‹ {url_to_process} çš„å…§å®¹...'):
            result_html = start_full_crawler(url_to_process)
            
            if result_html:
                st.balloons() # æˆåŠŸå¾Œå™´èŠ±ç‰¹æ•ˆ
                html_bytes = result_html.encode('utf-8')
                st.download_button(
                    label="ğŸ“¥ é»æˆ‘å„²å­˜ HTML åˆè¼¯æª”æ¡ˆ",
                    data=html_bytes,
                    file_name=f"MacaoDaily_{today.strftime('%Y%m%d')}.html",
                    mime="text/html"
                )
    else:
        st.warning("è«‹å…ˆè¼¸å…¥ç¶²å€æˆ–é»æ“Šç•¶å¤©æŒ‰éˆ•")
