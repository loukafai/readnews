import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import time
import io

# è¨­å®šç¶²é è³‡è¨Š
st.set_page_config(page_title="æ¾³é–€æ—¥å ±ä¸‹è¼‰å™¨", page_icon="ğŸ‡²ğŸ‡´")

def start_full_crawler(target_url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0"
    }

    try:
        res = requests.get(target_url, headers=headers, timeout=15)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # æå–æ–‡ç« é€£çµ
        links = []
        for a in soup.find_all('a', href=True):
            if 'content_' in a['href']:
                links.append(urljoin(target_url, a['href']))
        
        article_links = list(dict.fromkeys(links))
        total = len(article_links)
        
        if total == 0:
            st.error("âŒ æ‰¾ä¸åˆ°æ–‡ç« é€£çµï¼Œè«‹æª¢æŸ¥ç¶²å€ã€‚")
            return None

        # æº–å‚™é€²åº¦æ¢
        progress_bar = st.progress(0)
        status_text = st.empty()

        # æª”åè™•ç†
        date_match = re.search(r'(\d{4}-\d{2}/\d{2})', target_url)
        date_id = date_match.group(1).replace('-', '').replace('/', '') if date_match else "Archive"

        # HTML æ¨¡æ¿
        html_start = f"""<!DOCTYPE html><html lang="zh-Hant"><head><meta charset="UTF-8">
        <style>
            body {{ font-family: sans-serif; line-height: 1.8; max-width: 800px; margin: 0 auto; padding: 20px; background: #f4f4f4; }}
            .article-card {{ background: white; padding: 30px; margin-bottom: 30px; border-radius: 8px; shadow: 0 2px 5px rgba(0,0,0,0.1); }}
            .news-title {{ color: #aa0000; font-size: 1.8em; font-weight: bold; }}
            .news-image {{ max-width: 100%; display: block; margin: 20px auto; border-radius: 4px; }}
            #toc {{ background: white; padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid #ddd; }}
        </style></head><body><h1>æ¾³é–€æ—¥å ±åˆè¼¯ ({date_id})</h1><div id="toc"><h3>ğŸ“‹ ç›®éŒ„</h3>"""

        toc_html = ""
        articles_body = ""

        for i, link in enumerate(article_links):
            try:
                status_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {link.split('/')[-1]}")
                r = requests.get(link, headers=headers, timeout=10)
                r.encoding = 'utf-8'
                raw_html = r.text

                # æ–¹æ­£æ¨™ç±¤æå–æ¨™é¡Œ
                title_match = re.search(r'<founder-title>(.*?)</founder-title>', raw_html, re.DOTALL)
                final_title = title_match.group(1).strip() if title_match else "ç„¡æ¨™é¡Œ"
                final_title = final_title.replace('<![CDATA[', '').replace(']]>', '')

                a_soup = BeautifulSoup(raw_html, 'html.parser')
                
                imgs_html = ""
                for img in a_soup.find_all('img'):
                    src = img.get('src')
                    if src and '/res/' in src:
                        full_img_url = urljoin(link, src)
                        imgs_html += f'<img src="{full_img_url}" class="news-image">'

                content_div = a_soup.find(id="ozoom")
                content_html = str(content_div) if content_div else "<p>ï¼ˆå…§æ–‡æ“·å–å¤±æ•—ï¼‰</p>"

                anchor_id = f"news_{i}"
                toc_html += f'<a href="#{anchor_id}" style="display:block;margin:5px 0;text-decoration:none;color:#0056b3;">{i+1}. {final_title}</a>'
                articles_body += f'<div class="article-card" id="{anchor_id}"><div class="news-title">{final_title}</div><hr>{imgs_html}<div>{content_html}</div></div>'
                
                progress_bar.progress((i + 1) / total)
                time.sleep(0.1)
            except:
                continue

        status_text.text("âœ¨ è™•ç†å®Œæˆï¼è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ä¸‹è¼‰ã€‚")
        return html_start + toc_html + "</div>" + articles_body + "</body></html>"

    except Exception as e:
        st.error(f"å´©æ½°: {e}")
        return None

# --- UI ä»‹é¢ ---
st.title("ğŸ‡²ğŸ‡´ æ¾³é–€æ—¥å ±å…¨ç‰ˆé¢ä¸‹è¼‰å™¨ v0.1")
st.info("è«‹è¼¸å…¥ç•¶å¤©æŸä¸€ç‰ˆçš„ node ç¶²å€ï¼Œç¨‹å¼æœƒè‡ªå‹•æŠ“å–è©²ç‰ˆæ•´é å…§å®¹ã€‚")

url_input = st.text_input("ç‰ˆé¢ç¶²å€:", value="https://www.macaodaily.com/html/2026-02/10/node_1.htm")

if st.button("ğŸš€ é–‹å§‹åˆ†æä¸¦ç”Ÿæˆåˆè¼¯"):
    with st.spinner('æ­£åœ¨æ¬é‹ä¸­ï¼Œè«‹ç¨å€™...'):
        result_html = start_full_crawler(url_input)
        
        if result_html:
            # å°‡çµæœè½‰ç‚ºå¯ä¸‹è¼‰çš„ byte æµ
            html_bytes = result_html.encode('utf-8')
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ HTML åˆè¼¯æª”æ¡ˆ",
                data=html_bytes,
                file_name="MacaoDaily_Export.html",
                mime="text/html"
            )
