import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import time
import io
import datetime

# è¨­å®šç¶²é è³‡è¨Š
st.set_page_config(page_title="æ¾³é–€æ—¥å ±ä¸‹è¼‰å™¨", page_icon="ğŸ‡²ğŸ‡´")

def start_full_crawler(target_url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0"
    }

    try:
        res = requests.get(target_url, headers=headers, timeout=15)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # æå–æ–‡ç« é€£çµ
        links = []
        for a in soup.find_all('a', href=True):
            if 'content_' in a['href']:
                links.append(urljoin(target_url, a['href']))
        
        article_links = list(dict.fromkeys(links))
        total = len(article_links)
        
        if total == 0:
            st.error("âŒ æ‰¾ä¸åˆ°æ–‡ç« é€£çµï¼Œè«‹æª¢æŸ¥ç¶²å€ã€‚")
            return None

        # æº–å‚™é€²åº¦æ¢
        progress_bar = st.progress(0)
        status_text = st.empty()

        # æª”åè™•ç†
        date_match = re.search(r'(\d{4}-\d{2}/\d{2})', target_url)
        date_id = date_match.group(1).replace('-', '').replace('/', '') if date_match else "Archive"

        # HTML æ¨¡æ¿
        html_start = f"""<!DOCTYPE html><html lang="zh-Hant"><head><meta charset="UTF-8">
        <style>
            body {{ font-family: sans-serif; line-height: 1.8; max-width: 800px; margin: 0 auto; padding: 20px; background: #f4f4f4; }}
            .article-card {{ background: white; padding: 30px; margin-bottom: 30px; border-radius: 8px; shadow: 0 2px 5px rgba(0,0,0,0.1); }}
            .news-title {{ color: #aa0000; font-size: 1.8em; font-weight: bold; }}
            .news-image {{ max-width: 100%; display: block; margin: 20px auto; border-radius: 4px; }}
            #toc {{ background: white; padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid #ddd; }}
        </style></head><body><h1>æ¾³é–€æ—¥å ±åˆè¼¯ ({date_id})</h1><div id="toc"><h3>ğŸ“‹ ç›®éŒ„</h3>"""

        toc_html = ""
        articles_body = ""

        for i, link in enumerate(article_links):
            try:
                status_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {link.split('/')[-1]}")
                r = requests.get(link, headers=headers, timeout=10)
                r.encoding = 'utf-8'
                raw_html = r.text

                # æ–¹æ­£æ¨™ç±¤æå–æ¨™é¡Œ
                title_match = re.search(r'<founder-title>(.*?)</founder-title>', raw_html, re.DOTALL)
                final_title = title_match.group(1).strip() if title_match else "ç„¡æ¨™é¡Œ"
                final_title = final_title.replace('<![CDATA[', '').replace(']]>', '')

                a_soup = BeautifulSoup(raw_html, 'html.parser')
                
                imgs_html = ""
                for img in a_soup.find_all('img'):
                    src = img.get('src')
                    if src and '/res/' in src:
                        full_img_url = urljoin(link, src)
                        imgs_html += f'<img src="{full_img_url}" class="news-image">'

                content_div = a_soup.find(id="ozoom")
                content_html = str(content_div) if content_div else "<p>ï¼ˆå…§æ–‡æ“·å–å¤±æ•—ï¼‰</p>"

                anchor_id = f"news_{i}"
                toc_html += f'<a href="#{anchor_id}" style="display:block;margin:5px 0;text-decoration:none;color:#0056b3;">{i+1}. {final_title}</a>'
                articles_body += f'<div class="article-card" id="{anchor_id}"><div class="news-title">{final_title}</div><hr>{imgs_html}<div>{content_html}</div></div>'
                
                progress_bar.progress((i + 1) / total)
                time.sleep(0.1)
            except:
                continue

        status_text.text("âœ¨ è™•ç†å®Œæˆï¼è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ä¸‹è¼‰ã€‚")
        return html_start + toc_html + "</div>" + articles_body + "</body></html>"

    except Exception as e:
        st.error(f"å´©æ½°: {e}")
        return None

# --- UI ä»‹é¢ ---
st.title("ğŸ‡²ğŸ‡´ æ¾³é–€æ—¥å ±å…¨ç‰ˆé¢ä¸‹è¼‰å™¨ v0.2")

# ç²å–ä»Šå¤©æ—¥æœŸ
today = datetime.date.today()
formatted_date = today.strftime("%Y-%m/%d")
today_url = f"https://www.macaodaily.com/html/{formatted_date}/node_1.htm"

# å»ºç«‹åŠŸèƒ½å€å¡Š
st.info(f"ğŸ“… ä»Šå¤©çš„å»ºè­°ç¶²å€: {today_url}")

# å»ºç«‹æŒ‰éˆ•æ¬„ä½
col1, col2 = st.columns(2)

target_url = "" # ç”¨ä¾†æ¥æ”¶æœ€çµ‚è¦åŸ·è¡Œçš„ç¶²å€
trigger_start = False # ç”¨ä¾†æ¨™è¨˜æ˜¯å¦é–‹å§‹åŸ·è¡Œ

with col1:
    if st.button("ğŸ“… ä¸‹è¼‰ç•¶å¤©æ–°è", type="primary", use_container_width=True):
        target_url = today_url
        trigger_start = True

with col2:
    # è®“ç”¨æˆ¶ä¹Ÿå¯ä»¥æ‰‹å‹•è¼¸å…¥
    manual_url = st.text_input("æˆ–æ‰‹å‹•è¼¸å…¥ç¶²å€:", placeholder="https://...", label_visibility="collapsed")
    if st.button("ğŸš€ é–‹å§‹åˆ†ææ‰‹å‹•ç¶²å€", use_container_width=True):
        target_url = manual_url
        trigger_start = True

# --- æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
if trigger_start:
    if target_url:
        with st.spinner(f'æ­£åœ¨è§£æ: {target_url}'):
            result_html = start_full_crawler(target_url)
            
            if result_html:
                st.success("âœ… ç”Ÿæˆå®Œæˆï¼")
                st.balloons()
                
                # ä¸‹è¼‰æŒ‰éˆ•
                st.download_button(
                    label="ğŸ’¾ é»æˆ‘å„²å­˜ HTML æª”æ¡ˆ",
                    data=result_html.encode('utf-8'),
                    file_name=f"MacaoDaily_{target_url.split('/')[-1].replace('.htm', '.html')}",
                    mime="text/html",
                    use_container_width=True
                )
    else:
        st.warning("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„ç¶²å€ã€‚")
